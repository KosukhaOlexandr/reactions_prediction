{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KosukhaOlexandr/reactions_prediction/blob/main/rp_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmXfKT-8yM4j"
      },
      "source": [
        "# **Import packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Y3qTJxawsga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adaa34ca-098a-4f40-9c74-9a98ba40e97e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "import transformers\n",
        "from transformers import OpenAIGPTTokenizer, OpenAIGPTModel\n",
        "from transformers import BertModel, BertTokenizer, get_polynomial_decay_schedule_with_warmup\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from transformers import DebertaV2Tokenizer, DebertaV2Model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Su7UudBcKjxO",
        "outputId": "e4d7f1fc-08af-4748-9f4a-2aaa68e3de40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "concatenated_pos_neg_bert.csv  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW4v3hphyVBl"
      },
      "source": [
        "# **Data Preprocessing and visualizations**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eagh2GBSNudG"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "PIFKgQp-BFyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install gdown\n",
        "import gdown\n",
        "\n",
        "all_url = 'https://drive.google.com/uc?id=1Q3ghJ2b4gCtcVULpNIGMyuQhZn9rwMX8'\n",
        "tsn_url = 'https://drive.google.com/uc?id=1jGdkM0fJ9t64GNGIvQ9iydTfZxAc8QY8'\n",
        "all_bert_url = 'https://drive.google.com/uc?id=1EtYz5GcnunPQw3pUFQVlBuloxbazYrfH'\n",
        "output = \"concatenated_pos_neg_bert.csv\"\n",
        "gdown.download(all_bert_url, output, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "iuEdtDS62snf",
        "outputId": "be0a12e2-30d3-44e8-9dc5-e56fe25dfde0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EtYz5GcnunPQw3pUFQVlBuloxbazYrfH\n",
            "To: /content/concatenated_pos_neg_bert.csv\n",
            "100%|██████████| 41.1M/41.1M [00:00<00:00, 226MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'concatenated_pos_neg_bert.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h85TUUA-qQhG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('concatenated_pos_neg_bert.csv', index_col = 0)\n",
        "data.head(5)\n",
        "\n",
        "data = data.iloc[:1000,:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFH0uTnW3kVh",
        "outputId": "8bcbb686-a2d7-4db2-d1e2-9d36440bb634"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCfbXjjWKcMK"
      },
      "source": [
        "## Config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9tpheI4Khay"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "\n",
        "    # Model Config\n",
        "    model_name = \"microsoft/deberta-v2-xlarge\"\n",
        "    #can be\n",
        "    # openai-gpt\n",
        "    # roberta-base\n",
        "    # model/ for lang-uk bert\n",
        "    # microsoft/deberta-v2-xlarge for deberta\n",
        "    max_len = 512\n",
        "\n",
        "    hidden_size = 768\n",
        "    hidden_size2 = 512\n",
        "\n",
        "    batch_size = 16\n",
        "    if model_name == \"microsoft/deberta-v2-xlarge\":\n",
        "      max_len = 128\n",
        "      hidden_size = 1536\n",
        "      hidden_size2 = 2\n",
        "      batch_size = 1\n",
        "\n",
        "\n",
        "    # Data preparation\n",
        "    test_fraction = 0.1\n",
        "    validation_fraction = 0.1\n",
        "    num_workers = 0\n",
        "    classes = (1, 0)\n",
        "    tags_map = {cls:i for i,cls in enumerate(classes)}\n",
        "    logdir = 'logdir'\n",
        "\n",
        "    # Training\n",
        "    seed = 21\n",
        "    epochs = 10\n",
        "    learning_rate = 1e-5\n",
        "    num_classes = len(classes)\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ct9wTpKPKqP6"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "set_seed(Config.seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kv4V0AguCsh"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JD2-zL2orlaP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c24b736-e4f2-45c3-b57c-ffeb93d2f0aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
            "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import OpenAIGPTTokenizer, OpenAIGPTModel\n",
        "\n",
        "if Config.model_name == 'openai-gpt':\n",
        "  tokenizer = OpenAIGPTTokenizer.from_pretrained(Config.model_name, max_length=Config.max_len)\n",
        "  base_model = OpenAIGPTModel.from_pretrained(Config.model_name)\n",
        "elif Config.model_name == 'roberta-base':\n",
        "  tokenizer = RobertaTokenizer.from_pretrained(Config.model_name, truncation=True)\n",
        "  base_model = RobertaModel.from_pretrained(Config.model_name)\n",
        "elif Config.model_name == 'microsoft/deberta-v2-xlarge':\n",
        "  tokenizer = DebertaV2Tokenizer.from_pretrained(Config.model_name)\n",
        "  base_model = DebertaV2Model.from_pretrained(Config.model_name)\n",
        "else:\n",
        "  tokenizer = BertTokenizer.from_pretrained(Config.model_name)\n",
        "  base_model = BertModel.from_pretrained(Config.model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcEASe3K39lE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsPMTvO641sl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc6b9501-0741-4a99-ec44-1bd8c8656f1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence: оголошена повітряна тривога будь ласка перейдіть до укриттів\n",
            "Tokenized sentence: ['▁', 'ого', 'ло', 'ше', 'на', '▁по', 'в', 'і', 'тр', 'я', 'на', '▁', 'три', 'во', 'га', '▁буд', 'ь', '▁', 'ла', 'ска', '▁пере', 'йд', 'і', 'ть', '▁до', '▁у', 'кр', 'ит', 'т', 'ів']\n",
            "Token IDs: [250, 59058, 43908, 71786, 30460, 14826, 19464, 9395, 87730, 21570, 30460, 250, 113739, 51502, 52279, 118832, 35394, 250, 26565, 92592, 71312, 118697, 9395, 50556, 38933, 19798, 86114, 40396, 23183, 119356]\n"
          ]
        }
      ],
      "source": [
        "sample_text = 'оголошена повітряна тривога будь ласка перейдіть до укриттів'\n",
        "tokens = tokenizer.tokenize(sample_text)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(f'Original sentence: {sample_text}')\n",
        "print(f'Tokenized sentence: {tokens}')\n",
        "print(f'Token IDs: {token_ids}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxltI1mJ8aqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8343b9c-0930-4b3c-acfb-fd8522fc70d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: tensor([[     1,    250,  59058,  43908,  71786,  30460,  14826,  19464,   9395,\n",
            "          87730,  21570,  30460,    250, 113739,  51502,  52279, 118832,  35394,\n",
            "            250,  26565,  92592,  71312, 118697,   9395,  50556,  38933,  19798,\n",
            "          86114,  40396,  23183, 119356,      2]])\n",
            "Attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "Padded text length: 32\n"
          ]
        }
      ],
      "source": [
        "if Config.model_name == 'openai-gpt':\n",
        "  tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "encoding = tokenizer.encode_plus(\n",
        "  sample_text,\n",
        "  max_length=32,\n",
        "  add_special_tokens=True,\n",
        "  return_token_type_ids=False,\n",
        "  padding='max_length',\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',\n",
        ")\n",
        "\n",
        "input_ids = encoding['input_ids']\n",
        "attn_mask = encoding['attention_mask']\n",
        "ids_to_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "\n",
        "print(f'Input IDs: {input_ids}')\n",
        "print(f'Attention mask: {attn_mask}')\n",
        "print(f'Padded text length: {len(input_ids[0])}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bPN4cWJE6Tq"
      },
      "source": [
        "# **Choosing Sequence Length**\n",
        "\n",
        "BERT model works with fixed-length sequences. So, we need to choose the max length we will use.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_73uM8LD1fS"
      },
      "outputs": [],
      "source": [
        "# VALIDATION_FIELD[func] get_token_lens\n",
        "\n",
        "def get_token_lens(df):\n",
        "\n",
        "    token_lens = []\n",
        "\n",
        "    tk_len = df.apply(lambda x: len(tokenizer.encode(x.msg_text, max_length=Config.max_len, truncation=True)), axis = 1).to_numpy()\n",
        "    return tk_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrqvqGZKFmJf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e300a427-0bce-469d-a7c4-e4b49d44311e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token lens:  [ 87  92 128 128 128]\n"
          ]
        }
      ],
      "source": [
        "sample_df = data.iloc[:5].copy()\n",
        "sample_df_token_lens = get_token_lens(sample_df)\n",
        "print('Token lens: ', sample_df_token_lens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O40yxSMRG263"
      },
      "outputs": [],
      "source": [
        "def select_rows_with_required_token_lens(df, max_len=Config.max_len):\n",
        "    df['token_len'] = get_token_lens(df)\n",
        "    return df.loc[df.token_len < max_len].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KxbGNxUV8Gv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94e90047-b3e1-4b4e-e20d-a20b9e195192"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            msg_text  reaction_type  token_len\n",
              "0  Чий борщ? У п’ятницю, липня, ЮНЕСКО вирішить д...              1         87\n",
              "1  Зруйнований міст у Демидові на Київщині планую...              1         92"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e699f80e-383c-4117-9e5b-16f5a1dda144\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>msg_text</th>\n",
              "      <th>reaction_type</th>\n",
              "      <th>token_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Чий борщ? У п’ятницю, липня, ЮНЕСКО вирішить д...</td>\n",
              "      <td>1</td>\n",
              "      <td>87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Зруйнований міст у Демидові на Київщині планую...</td>\n",
              "      <td>1</td>\n",
              "      <td>92</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e699f80e-383c-4117-9e5b-16f5a1dda144')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e699f80e-383c-4117-9e5b-16f5a1dda144 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e699f80e-383c-4117-9e5b-16f5a1dda144');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "new_sample_df = select_rows_with_required_token_lens(sample_df)\n",
        "new_sample_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9QOFaRlLtll"
      },
      "source": [
        "# **Train/Test/Val Split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7N0TpGCiLujf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90ba79cd-d432-4e27-abf2-a8c88e03c7f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data: 392\n",
            "Valid data: 49\n",
            "Test data: 49\n"
          ]
        }
      ],
      "source": [
        "new_data = select_rows_with_required_token_lens(data.copy())\n",
        "\n",
        "train_to_rest = Config.validation_fraction + Config.test_fraction\n",
        "test_to_valid = Config.validation_fraction / train_to_rest\n",
        "\n",
        "train_df, rest = train_test_split(new_data, random_state=Config.seed, test_size=train_to_rest)\n",
        "\n",
        "test_df, valid_df = train_test_split(rest, random_state=Config.seed,test_size=test_to_valid)\n",
        "\n",
        "print('Train data:', train_df.shape[0])\n",
        "print('Valid data:', valid_df.shape[0])\n",
        "print('Test data:', test_df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTviR_k7UMMG"
      },
      "source": [
        "# **Dataset class and Dataloaders**\n",
        "We will define our custom PyTorch Dataset to load the quotes and their tags as one data sample.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQdU9VQ3ULhr"
      },
      "outputs": [],
      "source": [
        "class NewsDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df, max_len=Config.max_len):\n",
        "        self.tags = df.reaction_type.map(Config.tags_map).to_numpy()\n",
        "        self.quotes = df.msg_text.apply(\n",
        "            tokenizer.encode_plus, padding='max_length',\n",
        "                                  max_length=max_len, truncation=True,\n",
        "                                  return_tensors=\"pt\", return_attention_mask=True\n",
        "                              ).to_numpy()\n",
        "    def __len__(self):\n",
        "        return len(self.quotes)\n",
        "\n",
        "    def get_batch_tags(self, idx):\n",
        "        return self.tags[idx]\n",
        "\n",
        "    def get_batch_quotes(self, idx):\n",
        "        return self.quotes[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_quotes = self.get_batch_quotes(idx)\n",
        "        batch_y = self.get_batch_tags(idx)\n",
        "\n",
        "        return batch_quotes, batch_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-3bnCmY5GYG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb6bdb5d-aef1-4ebf-b7a7-62471bc7b449"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[     1,  66974,  86114,  31290,  23384, 119356,  62182,  67937, 116002,\n",
            "              2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "Input_ids: tensor([[     1,  66974,  86114,  31290,  23384, 119356,  62182,  67937, 116002,\n",
            "              2]])\n",
            "Attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "Edcoded tag: 0\n"
          ]
        }
      ],
      "source": [
        "news_ds = NewsDataset(sample_df, max_len=10)\n",
        "news_data, tag = news_ds[3]\n",
        "print(news_data)\n",
        "print(\"Input_ids:\", news_data['input_ids'])\n",
        "#print(\"Token_type_ids:\", quote_data['token_type_ids'])\n",
        "print(\"Attention_mask:\", news_data['attention_mask'])\n",
        "print(\"Edcoded tag:\", tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DCxp3g6T-Do"
      },
      "outputs": [],
      "source": [
        "train_data = NewsDataset(train_df)\n",
        "valid_data = NewsDataset(valid_df)\n",
        "test_data = NewsDataset(test_df)\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=Config.batch_size, shuffle=True, num_workers=2)\n",
        "valid_dataloader = DataLoader(valid_data, batch_size=Config.batch_size, shuffle=False, num_workers=2)\n",
        "test_dataloader = DataLoader(test_data, batch_size=Config.batch_size, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85lSKtxIWiH-"
      },
      "source": [
        "# **Creating the model class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDWKbXnfWc7R"
      },
      "outputs": [],
      "source": [
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, hidden_size1, hidden_size2, num_classes):\n",
        "        super(BertClassifier, self).__init__()\n",
        "        self.base_m = base_model\n",
        "        self.dropout = nn.Dropout(0.35)\n",
        "        self.fc1 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.fc2 = nn.Linear(hidden_size2, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def mean_pooling(self, model_output, attention_mask):\n",
        "        token_embeddings = model_output[0]  #First element of model_output contains all token embeddings\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "\n",
        "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "        seq_output = self.base_m(input_id, attention_mask=mask, return_dict=False)\n",
        "\n",
        "        text_emb = self.mean_pooling(seq_output, mask)\n",
        "\n",
        "        final_layer = self.fc1(text_emb)\n",
        "\n",
        "        final_layer = self.relu(final_layer)\n",
        "\n",
        "        final_layer = self.dropout(final_layer)\n",
        "\n",
        "        final_layer = self.fc2(final_layer)\n",
        "\n",
        "        final_layer = self.softmax(final_layer)\n",
        "\n",
        "        return final_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1fuQS2xv4BL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad93014-e6ef-4f11-d2ce-4908bde42d68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 32]) torch.Size([1, 32])\n",
            "\n",
            "Output: tensor([[-0.3710, -1.1712]], grad_fn=<LogSoftmaxBackward0>)\n",
            "Output shape: torch.Size([1, 2])\n"
          ]
        }
      ],
      "source": [
        "set_seed(Config.seed)\n",
        "bert_Classifier = BertClassifier(Config.hidden_size, Config.hidden_size2, Config.num_classes)\n",
        "print(input_ids.shape, attn_mask.shape)\n",
        "print('\\nOutput:', bert_Classifier(input_ids, attn_mask)) # input_ids: [1,32], attn_mask:[1,32]\n",
        "print('Output shape:', bert_Classifier(input_ids, attn_mask).shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_Classifier"
      ],
      "metadata": {
        "id": "AzOGBR0Ae-cg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c34e5fc8-b123-46a7-976c-ea5a1f04b69f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertClassifier(\n",
              "  (base_m): DebertaV2Model(\n",
              "    (embeddings): DebertaV2Embeddings(\n",
              "      (word_embeddings): Embedding(128100, 1536, padding_idx=0)\n",
              "      (LayerNorm): LayerNorm((1536,), eps=1e-07, elementwise_affine=True)\n",
              "      (dropout): StableDropout()\n",
              "    )\n",
              "    (encoder): DebertaV2Encoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-23): 24 x DebertaV2Layer(\n",
              "          (attention): DebertaV2Attention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (query_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
              "              (key_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
              "              (value_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaV2SelfOutput(\n",
              "              (dense): Linear(in_features=1536, out_features=1536, bias=True)\n",
              "              (LayerNorm): LayerNorm((1536,), eps=1e-07, elementwise_affine=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaV2Intermediate(\n",
              "            (dense): Linear(in_features=1536, out_features=6144, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): DebertaV2Output(\n",
              "            (dense): Linear(in_features=6144, out_features=1536, bias=True)\n",
              "            (LayerNorm): LayerNorm((1536,), eps=1e-07, elementwise_affine=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (rel_embeddings): Embedding(512, 1536)\n",
              "      (LayerNorm): LayerNorm((1536,), eps=1e-07, elementwise_affine=True)\n",
              "      (conv): ConvLayer(\n",
              "        (conv): Conv1d(1536, 1536, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (LayerNorm): LayerNorm((1536,), eps=1e-07, elementwise_affine=True)\n",
              "        (dropout): StableDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.35, inplace=False)\n",
              "  (fc1): Linear(in_features=1536, out_features=2, bias=True)\n",
              "  (fc2): Linear(in_features=2, out_features=2, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (softmax): LogSoftmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "zB7SFF5--Cqy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c0173be-d1be-4660-8c13-67c0c0be33d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics.classification import BinaryF1Score\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "oNqNRf5c1X8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QATgMF0fZKFC"
      },
      "source": [
        "# **Training the model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWzM5jdwZBWU"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, scheduler, criterion, train_loader, device=Config.device):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader, desc='Iterating over train data')\n",
        "\n",
        "    total_loss_train = 0\n",
        "    total_acc_train = 0\n",
        "    step_cuda = 0\n",
        "    f1_labels = []\n",
        "    f1_outputs = []\n",
        "    for train_input, train_label in pbar:\n",
        "        train_label = train_label.to(device)\n",
        "        mask = torch.squeeze(train_input['attention_mask'], 1).to(device)\n",
        "        input_id = torch.squeeze(train_input['input_ids'], 1).to(device)\n",
        "\n",
        "        # forward\n",
        "        model.zero_grad()\n",
        "\n",
        "        output = model(input_id, mask)\n",
        "        batch_loss = criterion(output, train_label)\n",
        "        batch_loss.backward()\n",
        "        total_loss_train += batch_loss.item()\n",
        "\n",
        "        out_max = torch.argmax(output, dim=1).cpu().detach().numpy()\n",
        "        acc = (out_max == train_label.cpu().detach().numpy()).sum()\n",
        "        total_acc_train += acc\n",
        "\n",
        "        f1_labels.append(train_label.cpu().detach().numpy())\n",
        "        f1_outputs.append(out_max)\n",
        "\n",
        "        # optimize\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    f1_labels = np.concatenate(f1_labels)\n",
        "    f1_outputs = np.concatenate(f1_outputs)\n",
        "    print(f1_labels, f1_outputs)\n",
        "    f1 = f1_score(f1_labels, f1_outputs)\n",
        "\n",
        "    return total_loss_train/ len(train_loader.dataset), total_acc_train/ len(train_loader.dataset), f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "299OfqWIcAsa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "0954fae1-0a8e-48fb-d7a5-2d2492da36e2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-65ccbe34f07e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# try stochastic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m scheduler = get_polynomial_decay_schedule_with_warmup(optimizer, num_warmup_steps=4,\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1143\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m   1142\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m-> 1143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 752.00 MiB (GPU 0; 14.75 GiB total capacity; 12.95 GiB already allocated; 16.81 MiB free; 13.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "set_seed(Config.seed)\n",
        "model = BertClassifier(Config.hidden_size, Config.hidden_size2, Config.num_classes).to(Config.device)\n",
        "optimizer = Adam(model.parameters(), lr=Config.learning_rate)\n",
        "# try stochastic\n",
        "scheduler = get_polynomial_decay_schedule_with_warmup(optimizer, num_warmup_steps=4,\n",
        "              num_training_steps=int(len(train_dataloader) * Config.epochs) , power=2)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "loss, acc, f1 = train(model, optimizer, scheduler ,criterion, test_dataloader)\n",
        "print(f'\\nTrain Loss: {loss: .3f} | Train Accuracy: {acc: .3f} | Train F1-score: {acc: .3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjE3PCaDfSqh"
      },
      "source": [
        "## Evaluation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ws8I7YebccvE"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, criterion, eval_loader, device=Config.device):\n",
        "    model.eval()\n",
        "\n",
        "    total_acc_val = 0\n",
        "    total_loss_val = 0\n",
        "\n",
        "    f1_labels = []\n",
        "    f1_outputs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(eval_loader, desc='Iterating over evaluation data')\n",
        "        for val_input, val_label in pbar:\n",
        "            # pass to device\n",
        "            val_label = val_label.to(device)\n",
        "            mask = torch.squeeze(val_input['attention_mask'], 1).to(device)\n",
        "            input_id = torch.squeeze(val_input['input_ids'], 1).to(device)\n",
        "\n",
        "            # forward\n",
        "\n",
        "            output = model(input_id, mask)\n",
        "\n",
        "            batch_loss = criterion(output, val_label).item()      #calculate loss\n",
        "            total_loss_val += batch_loss                   # += loss\n",
        "\n",
        "            out_max = torch.argmax(output, dim=1).cpu().detach().numpy()\n",
        "            acc = (out_max == val_label.cpu().detach().numpy()).sum()    # calculate accuracy\n",
        "            total_acc_val += acc                    # += acc\n",
        "\n",
        "            f1_labels.append(val_label.cpu().detach().numpy())\n",
        "            f1_outputs.append(out_max)\n",
        "\n",
        "    f1_labels = np.concatenate(f1_labels)\n",
        "    f1_outputs = np.concatenate(f1_outputs)\n",
        "\n",
        "    f1 = f1_score(f1_labels, f1_outputs)\n",
        "    return total_loss_val/ len(eval_loader.dataset), total_acc_val/ len(eval_loader.dataset), f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JbniIAWgJ-w"
      },
      "outputs": [],
      "source": [
        "\"\"\"set_seed(Config.seed)\n",
        "model = BertClassifier(Config.hidden_size, Config.num_classes).to(Config.device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#loss, acc = evaluate(model, criterion, valid_dataloader)\n",
        "#print(f'\\nEval Loss: {loss: .3f} | Eval Accuracy: {acc: .3f}')\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3yvQzYBghHY"
      },
      "outputs": [],
      "source": [
        "def train_loop(model, optimizer, scheduler, criterion, train_loader, valid_loader, device=Config.device, num_epochs=Config.epochs, logdir=Config.logdir):\n",
        "\n",
        "    history = defaultdict(list)\n",
        "    best_accuracy = 0\n",
        "\n",
        "    tb_writer = SummaryWriter(log_dir=logdir)\n",
        "    for e in range(num_epochs):\n",
        "\n",
        "        print(f'Epoch {e + 1}/{num_epochs}')\n",
        "\n",
        "        # train on training set\n",
        "        train_loss, train_acc, train_f1 = train(model, optimizer, scheduler, criterion, train_loader, device=device)\n",
        "        # evaluate on validation set\n",
        "        val_loss, val_acc, val_f1 = evaluate(model, criterion, valid_loader, device=device)\n",
        "\n",
        "        print(f'\\nTrain Loss {train_loss: .3f} | Val Loss {val_loss: .3f}')\n",
        "        print(f'Train Accuracy {train_acc: .3f} | Val Accuracy {val_acc: .3f}')\n",
        "        print(f'Train F1-score {train_f1: .3f} | Val F1-Score {val_f1: .3f}')\n",
        "        print()\n",
        "\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_f1'].append(train_f1)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_f1'].append(val_f1)\n",
        "\n",
        "        # Tensorboards Logging\n",
        "        tb_writer.add_scalar('Bert/Train Loss', train_loss, e)\n",
        "        tb_writer.add_scalar('Bert/Valid Loss', val_loss, e)\n",
        "        tb_writer.add_scalar('Bert/Train Accuracy', train_acc, e)\n",
        "        tb_writer.add_scalar('Bert/Valid Accuracy', val_acc, e)\n",
        "        tb_writer.add_scalar('Bert/Train F1-score', train_f1, e)\n",
        "        tb_writer.add_scalar('Bert/Val F1-score', val_f1, e)\n",
        "\n",
        "        if val_acc > best_accuracy:\n",
        "            torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "            best_accuracy = val_acc\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UU9VLxsgjQ6N"
      },
      "outputs": [],
      "source": [
        "set_seed(Config.seed)\n",
        "model = BertClassifier(Config.hidden_size, Config.hidden_size2, Config.num_classes).to(Config.device)\n",
        "optimizer = Adam(model.parameters(), lr=Config.learning_rate)\n",
        "\n",
        "scheduler = get_polynomial_decay_schedule_with_warmup(optimizer,\n",
        "                                  num_warmup_steps=4,\n",
        "                                  num_training_steps=int(len(train_dataloader) * Config.epochs) , power=2)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "history = train_loop(model, optimizer, scheduler, criterion, train_dataloader, valid_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1PN_c4h6pik"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=logdir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzW1GgwILjEg"
      },
      "outputs": [],
      "source": [
        "fig, ax =  plt.subplots(2, 1, figsize=(10, 6))\n",
        "ax[0].plot(history['train_acc'], label='train accuracy')\n",
        "ax[0].plot(history['val_acc'], label='validation accuracy')\n",
        "ax[0].set_title('Accuracy')\n",
        "ax[0].legend()\n",
        "ax[1].plot(history['train_loss'], label='train loss')\n",
        "ax[1].plot(history['val_loss'], label='validation loss')\n",
        "ax[1].set_title('Losses')\n",
        "ax[1].legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_gQ12CMqY8t"
      },
      "outputs": [],
      "source": [
        "# Load the latest model\n",
        "model.load_state_dict(torch.load('best_model_state.bin'))\n",
        "loss, acc, f1 = evaluate(model, criterion, test_dataloader)\n",
        "\n",
        "print(f'\\nTest Loss: {loss : .3f} | Test Accuracy {acc : .3f} | Test F1-Score {f1 : .3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFz1DuEDyfBp"
      },
      "source": [
        "# **Predicting on Raw Text**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4b6LQbZyeiU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QsX8fwHfYVd"
      },
      "source": [
        "Use the tokenizer to encode the text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewjvqLuPxxX8"
      },
      "outputs": [],
      "source": [
        "encoded_quote = tokenizer.encode_plus(\n",
        "  quote_text,\n",
        "  max_length=Config.max_len,\n",
        "  add_special_tokens=True,\n",
        "  return_token_type_ids=False,\n",
        "  padding='max_length',\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqguIZSufkA1"
      },
      "source": [
        "Get the predictions from our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5a-_RfBzBnI"
      },
      "outputs": [],
      "source": [
        "input_ids = encoded_quote['input_ids'].to(Config.device)\n",
        "attention_mask = encoded_quote['attention_mask'].to(Config.device)\n",
        "\n",
        "output = model(input_ids, attention_mask)\n",
        "_, prediction = torch.max(output, dim=1)\n",
        "\n",
        "print(f'Review text: {quote_text}')\n",
        "print(f'Quotes tag: {Config.classes[prediction]}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6bPN4cWJE6Tq"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}